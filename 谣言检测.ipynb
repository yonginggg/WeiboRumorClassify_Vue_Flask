{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入词向量模型，https://github.com/Embedding/Chinese-Word-Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cn_model = KeyedVectors.load_word2vec_format('./embeddings/sgns.weibo.bigram', \n",
    "                                             binary=False,\n",
    "                                             unicode_errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "666\n",
      "(300,)\n",
      "[ 0.867238  0.447538 -0.491654  0.442702  0.3249   -0.135766 -0.040395\n",
      " -0.004786 -0.542705  0.473954 -0.087931  0.182698 -0.513613  0.468792\n",
      "  0.414553  0.623913 -0.409261 -0.469407  0.514468 -0.918192 -0.359212\n",
      " -0.827985  0.474507  0.150142  0.21658   0.598472 -1.086412  0.492395\n",
      "  0.741502  1.547145 -0.094865  0.318981 -0.01839  -0.410346 -0.449421\n",
      "  0.148408  0.723303  0.504586  0.969177  0.276921 -0.1546   -0.646725\n",
      " -0.534673  0.905004  0.544035 -0.060806 -0.958402 -1.208919  0.5922\n",
      "  0.007131 -0.072191  0.461774  0.70442   0.669932  0.091214 -1.372572\n",
      " -0.262632 -0.21109  -0.204028  0.773246 -0.257194  0.018511 -0.320097\n",
      "  0.878415 -0.310128 -0.234066  0.032051  0.122899  0.215627  0.026293\n",
      "  0.643676  0.119276 -0.8249   -0.01256  -0.516165  0.1447    0.302147\n",
      " -0.372154  0.149966 -0.677081  0.186576  0.717744  0.631457  0.328822\n",
      " -0.404428  0.051025  1.094605  0.154441  0.315256  0.43526   0.546499\n",
      "  0.232901 -0.792701  0.644786  1.588248 -0.143094  0.900562 -0.469408\n",
      "  0.614301  0.099979  0.822639 -0.492338  0.130308  0.038273  0.16267\n",
      " -0.24818  -1.081412 -0.850202  0.203182  0.481128 -0.35068   1.25519\n",
      " -0.750094 -1.089396  0.888077  0.480552  0.051004 -0.861584  0.35957\n",
      " -0.317772 -0.811181  0.578266  0.225231  0.15672   0.331069 -0.144148\n",
      "  0.225306  0.304561 -0.249225  0.134652  0.52652   1.06446  -0.191233\n",
      "  0.544106 -0.071556 -0.088549  0.032077 -0.325895  0.511773 -0.518053\n",
      " -0.875682 -1.277643  0.127743  0.634704  0.277224  0.4093    0.77887\n",
      "  0.066587  1.073671  1.081905  0.615512  0.398345 -0.146392 -0.469875\n",
      " -0.154641 -0.040309 -0.134425 -0.675766  0.289368  0.759959  0.695076\n",
      " -0.957867  0.051999  0.296306  0.135402  0.227287  0.014893 -0.532533\n",
      "  0.446928 -0.684926 -0.126684  0.560691 -1.172543  0.521436  0.824754\n",
      " -0.337699  0.134216  0.59868  -0.409354 -0.467627  0.011302 -0.132495\n",
      " -0.34127   0.120249  0.348111  0.948581 -0.655243  0.525027 -0.152691\n",
      " -0.08505  -1.004684  0.032354 -1.125064 -0.505567 -1.097251  0.911653\n",
      "  0.025914  0.025328 -0.261008  0.555704  0.790003 -0.851178  0.584994\n",
      " -1.759093  0.536954  0.353756 -0.68643  -0.104768 -0.242855  0.053806\n",
      "  0.366697 -0.059632  0.983469  0.290523 -0.096193 -0.065079  0.339606\n",
      " -1.346052  1.400181  0.259377  0.055832 -0.153933 -0.036517  0.298418\n",
      "  0.147009  0.268625 -0.569361  0.65744  -0.028724  0.136583  2.153326\n",
      "  0.17036   0.115346 -0.077956 -0.460571 -0.576453  0.167189 -0.996222\n",
      "  0.287148  0.130513  0.460643 -0.724315  0.691318 -0.45234   0.120368\n",
      "  0.204849  0.40405  -0.256665  0.355974 -0.004295 -0.679281 -0.157419\n",
      " -1.022355 -0.047134 -0.235541 -0.131358 -0.012845 -0.533136 -0.012363\n",
      "  0.178782  0.848715  0.59895   0.345376 -0.513977  0.726109 -0.745971\n",
      "  1.025572  0.312063  0.554938  0.490135 -0.152596  0.05966  -0.807085\n",
      " -0.40297  -0.248144  0.78479  -1.431856 -0.670137  0.223885  0.176321\n",
      "  0.377732  0.028196 -0.357157  0.539969 -0.120427 -0.00299  -0.120715\n",
      "  0.104306 -0.388705  0.30674  -0.102421  0.624896 -0.534073  1.024752\n",
      " -0.223473  1.12147   0.224236  0.712388  0.062904  0.67496 ]\n"
     ]
    }
   ],
   "source": [
    "print(cn_model.vocab['心情'].index)\n",
    "print(cn_model.vectors[666].shape)\n",
    "print(cn_model.vectors[666])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_not_rumor</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>【李登辉今天凌晨心脏病复发身亡】台北快讯：原国民党、台联党主席，有“台独教父”之称的李登辉，...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>有一男生，平时老在一起玩，关系很好，喜欢他很久了。昨天表白，跟他说喜欢他，他笑了一下问我，喜...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>【央视员工爆料：广西惊现帝王局长】 广西都安民政局长黄某一个人吃着509份底保，九套房子，6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>朋友们远离穿这种图案服装的人，发给你的家人、爱人、朋友，如果见到请立即报警。 ​</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>中国好声音都长得像xx什么的。。。（精选超像+搞笑） 来自 人人网 李梓瑶。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_not_rumor                                            content\n",
       "0             0  【李登辉今天凌晨心脏病复发身亡】台北快讯：原国民党、台联党主席，有“台独教父”之称的李登辉，...\n",
       "1             1  有一男生，平时老在一起玩，关系很好，喜欢他很久了。昨天表白，跟他说喜欢他，他笑了一下问我，喜...\n",
       "2             0  【央视员工爆料：广西惊现帝王局长】 广西都安民政局长黄某一个人吃着509份底保，九套房子，6...\n",
       "3             0           朋友们远离穿这种图案服装的人，发给你的家人、爱人、朋友，如果见到请立即报警。 ​\n",
       "4             1           中国好声音都长得像xx什么的。。。（精选超像+搞笑） 来自 人人网 李梓瑶。  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weibo = pd.read_csv('./data/all_data.txt',sep='\\t', names=['is_not_rumor','content'],encoding='utf-8')\n",
    "weibo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "content = weibo.content.values.tolist()\n",
    "label=weibo.is_not_rumor.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0\\t【李登辉今天凌晨心脏病复发身亡】台北快讯：原国民党、台联党主席，有“台独教父”之称的李登辉，与今天凌晨在其家中因心脏病复发抢救无效死亡。'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(label[0])+'\\t'+content[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分词和tokenize，https://github.com/lancopku/PKUSeg-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pkuseg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords=pd.read_csv(\"./stopwords/stopwords.txt\",index_col=False,sep=\"\\t\",quoting=3,names=['stopword'], encoding='utf-8')\n",
    "stopwords = stopwords.stopword.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seg = pkuseg.pkuseg(model_name='web')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_tokens = []\n",
    "for text in content:\n",
    "    text = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、~@#￥%……&*（）]+\", \"\",text)\n",
    "    cut_list = seg.cut(text)\n",
    "    cut_list_clean=[]\n",
    "    for word in cut_list:\n",
    "        if word in stopwords:\n",
    "            continue\n",
    "        cut_list_clean.append(word)\n",
    "    \n",
    "    #索引化\n",
    "    for i, word in enumerate(cut_list_clean):\n",
    "        try:\n",
    "            # 将词转换为索引index\n",
    "            cut_list_clean[i] = cn_model.vocab[word].index\n",
    "        except KeyError:\n",
    "            # 如果词不在字典中，则输出0\n",
    "            cut_list_clean[i] = 0\n",
    "    train_tokens.append(cut_list_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获得所有tokens的长度\n",
    "num_tokens = [len(tokens) for tokens in train_tokens]\n",
    "num_tokens = np.array(num_tokens)\n",
    "# 取tokens平均值并加上两个tokens的标准差，\n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "max_tokens = int(max_tokens)\n",
    "max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_pad = pad_sequences(train_tokens, maxlen=max_tokens,\n",
    "                            padding='pre', truncating='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0, ..., 14638, 12594,  2143],\n",
       "       [    0,     0,     0, ...,   120,   120,  2664],\n",
       "       [    0,     0,     0, ..., 19557,    68, 27912],\n",
       "       ...,\n",
       "       [    0,     0,     0, ...,  1289,     0,   696],\n",
       "       [    0,     0,     0, ...,   569,   791,     0],\n",
       "       [    0,     0,     0, ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 生成词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 50000 #选择使用前50k个使用频率最高的词\n",
    "embedding_dim=300 #每一个词汇都用一个长度为300的向量表示\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "for i in range(num_words):\n",
    "    embedding_matrix[i,:] = cn_model[cn_model.index2word[i]]#前50000个index对应的词的词向量\n",
    "embedding_matrix = embedding_matrix.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_pad[train_pad>=num_words ] = 0\n",
    "train_target = np.array(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GRU, Embedding, LSTM, Bidirectional\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_pad,\n",
    "                                                    train_target,\n",
    "                                                    test_size=0.1,\n",
    "                                                    random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <function standard_lstm at 0x0000018405478488> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_lstm at 0x0000018405478488>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function standard_lstm at 0x0000018405478488> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_lstm at 0x0000018405478488>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <function cudnn_lstm at 0x0000018405478510> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_lstm at 0x0000018405478510>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function cudnn_lstm at 0x0000018405478510> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_lstm at 0x0000018405478510>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <function standard_lstm at 0x0000018405478488> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_lstm at 0x0000018405478488>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function standard_lstm at 0x0000018405478488> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_lstm at 0x0000018405478488>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <function cudnn_lstm at 0x0000018405478510> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_lstm at 0x0000018405478510>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function cudnn_lstm at 0x0000018405478510> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_lstm at 0x0000018405478510>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <function standard_lstm at 0x0000018405478488> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_lstm at 0x0000018405478488>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function standard_lstm at 0x0000018405478488> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_lstm at 0x0000018405478488>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <function cudnn_lstm at 0x0000018405478510> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_lstm at 0x0000018405478510>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function cudnn_lstm at 0x0000018405478510> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_lstm at 0x0000018405478510>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <function standard_lstm at 0x0000018405478488> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_lstm at 0x0000018405478488>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function standard_lstm at 0x0000018405478488> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_lstm at 0x0000018405478488>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <function cudnn_lstm at 0x0000018405478510> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_lstm at 0x0000018405478510>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function cudnn_lstm at 0x0000018405478510> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_lstm at 0x0000018405478510>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    }
   ],
   "source": [
    "#序贯(Sequential)模型\n",
    "model = Sequential()\n",
    "# 嵌入层\n",
    "model.add(Embedding(num_words,\n",
    "                    embedding_dim,\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=max_tokens,\n",
    "                    trainable=False))\n",
    "#Bidirectional包装器:双向RNN包装器\n",
    "model.add(Bidirectional(LSTM(units=64, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(units=32, return_sequences=False)))\n",
    "#全连接层\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "optimizer=Adam(lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 建立一个权重的存储点\n",
    "checkpoint_save_path=\"./checkpoint/rumor_LSTM.ckpt\"\n",
    "if os.path.exists(checkpoint_save_path+'.index'):\n",
    "    print('----------load the model----------')\n",
    "    model.load_weights(checkpoint_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#保存参数和模型\n",
    "checkpoint = ModelCheckpoint(filepath=checkpoint_save_path, monitor='val_loss',\n",
    "                                      verbose=1, save_weights_only=True,\n",
    "                                      save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 5个epoch内validation loss没有改善则停止训练\n",
    "earlystopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "\n",
    "# 自动降低learning rate\n",
    "lr_reduction = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                       factor=0.1, min_lr=1e-8, patience=0,\n",
    "                                       verbose=1)\n",
    "# 定义callback函数\n",
    "callbacks = [\n",
    "    earlystopping, \n",
    "#    checkpoint,\n",
    "    lr_reduction\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\CodeProgramFiles\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 2744 samples, validate on 305 samples\n",
      "Epoch 1/20\n",
      "2744/2744 [==============================] - 10s 4ms/sample - loss: 0.6374 - accuracy: 0.6337 - val_loss: 0.5225 - val_accuracy: 0.7738\n",
      "Epoch 2/20\n",
      "2688/2744 [============================>.] - ETA: 0s - loss: 0.4852 - accuracy: 0.7734\n",
      "Epoch 00002: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "2744/2744 [==============================] - 5s 2ms/sample - loss: 0.4819 - accuracy: 0.7751 - val_loss: 0.5350 - val_accuracy: 0.7082\n",
      "Epoch 3/20\n",
      "2744/2744 [==============================] - 5s 2ms/sample - loss: 0.4440 - accuracy: 0.7948 - val_loss: 0.4595 - val_accuracy: 0.8000\n",
      "Epoch 4/20\n",
      "2744/2744 [==============================] - 5s 2ms/sample - loss: 0.4060 - accuracy: 0.8258 - val_loss: 0.4404 - val_accuracy: 0.8033\n",
      "Epoch 5/20\n",
      "2744/2744 [==============================] - 5s 2ms/sample - loss: 0.3878 - accuracy: 0.8378 - val_loss: 0.4310 - val_accuracy: 0.8131\n",
      "Epoch 6/20\n",
      "2744/2744 [==============================] - 5s 2ms/sample - loss: 0.3760 - accuracy: 0.8473 - val_loss: 0.4223 - val_accuracy: 0.8197\n",
      "Epoch 7/20\n",
      "2744/2744 [==============================] - 5s 2ms/sample - loss: 0.3653 - accuracy: 0.8484 - val_loss: 0.4042 - val_accuracy: 0.8361\n",
      "Epoch 8/20\n",
      "2744/2744 [==============================] - 5s 2ms/sample - loss: 0.3547 - accuracy: 0.8509 - val_loss: 0.3999 - val_accuracy: 0.8393\n",
      "Epoch 9/20\n",
      "2744/2744 [==============================] - 5s 2ms/sample - loss: 0.3442 - accuracy: 0.8622 - val_loss: 0.3887 - val_accuracy: 0.8492\n",
      "Epoch 10/20\n",
      "2744/2744 [==============================] - 5s 2ms/sample - loss: 0.3378 - accuracy: 0.8608 - val_loss: 0.3825 - val_accuracy: 0.8492\n",
      "Epoch 11/20\n",
      "2688/2744 [============================>.] - ETA: 0s - loss: 0.3260 - accuracy: 0.8687\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "2744/2744 [==============================] - 5s 2ms/sample - loss: 0.3254 - accuracy: 0.8688 - val_loss: 0.3857 - val_accuracy: 0.8492\n",
      "Epoch 12/20\n",
      "2688/2744 [============================>.] - ETA: 0s - loss: 0.3196 - accuracy: 0.8731\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "2744/2744 [==============================] - 5s 2ms/sample - loss: 0.3170 - accuracy: 0.8750 - val_loss: 0.3834 - val_accuracy: 0.8525\n",
      "Epoch 13/20\n",
      "2688/2744 [============================>.] - ETA: 0s - loss: 0.3170 - accuracy: 0.8761\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "2744/2744 [==============================] - 5s 2ms/sample - loss: 0.3157 - accuracy: 0.8768 - val_loss: 0.3837 - val_accuracy: 0.8525\n",
      "Epoch 14/20\n",
      "2688/2744 [============================>.] - ETA: 0s - loss: 0.3167 - accuracy: 0.8754\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "2744/2744 [==============================] - 5s 2ms/sample - loss: 0.3156 - accuracy: 0.8765 - val_loss: 0.3836 - val_accuracy: 0.8525\n",
      "Epoch 15/20\n",
      "2688/2744 [============================>.] - ETA: 0s - loss: 0.3155 - accuracy: 0.8765\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1e-08.\n",
      "2744/2744 [==============================] - 5s 2ms/sample - loss: 0.3156 - accuracy: 0.8765 - val_loss: 0.3836 - val_accuracy: 0.8525\n",
      "Epoch 00015: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x18410c4a3c8>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train,validation_split=0.1,epochs=20,batch_size=128,callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('LSTM_rumor_model_58.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "339/339 [==============================] - 3s 7ms/sample - loss: 0.3195 - accuracy: 0.8761\n",
      "Accuracy:87.61%\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(X_test, y_test)\n",
    "print('Accuracy:{0:.2%}'.format(result[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_rumor_LSTM(text,label):\n",
    "    print(text)\n",
    "    text = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、~@#￥%……&*（）]+\", \"\",text)\n",
    "    cut = seg.cut(text)\n",
    "\n",
    "    cut_clean=[]\n",
    "    for word in cut:\n",
    "        if word in stopwords:\n",
    "            continue\n",
    "        cut_clean.append(word)\n",
    "\n",
    "    for i, word in enumerate(cut_clean):\n",
    "        try:\n",
    "            cut_clean[i] = cn_model.vocab[word].index\n",
    "            if cut_clean[i] >= 50000:\n",
    "                cut_clean[i] = 0\n",
    "        except KeyError:\n",
    "            cut_clean[i] = 0\n",
    "\n",
    "    tokens_pad = pad_sequences([cut_clean], maxlen=max_tokens,\n",
    "                           padding='pre', truncating='pre')\n",
    "\n",
    "    dic={0:'谣言',1:'非谣言'}\n",
    "    result = model.predict(x=tokens_pad)\n",
    "    coef = result[0][0]\n",
    "    if coef >= 0.5:\n",
    "        print('真实是'+dic[label],'预测是非谣言','output=%.2f'%coef)\n",
    "    else:\n",
    "        print('真实是'+dic[label],'预测是谣言','output=%.2f'%coef)\n",
    "    print('---------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "兴仁县今天抢小孩没抢走，把孩子母亲捅了一刀，看见这车的注意了，真事，车牌号辽HFM055！！！！！赶紧散播！ 都别带孩子出去瞎转悠了 尤其别让老人自己带孩子出去 太危险了 注意了！！！！辽HFM055北京现代朗动，在各学校门口抢小孩！！！110已经 证实！！全市通缉！！\n",
      "真实是谣言 预测是谣言 output=0.10\n",
      "---------------------------------------------\n",
      "重庆真实新闻:2016年6月1日在重庆梁平县袁驿镇发生一起抢儿童事件，做案人三个中年男人，在三中学校到镇街上的一条小路上，把小孩直接弄晕(儿童是袁驿新幼儿园中班的一名学生)，正准备带走时被家长及时发现用棒子赶走了做案人，故此获救！请各位同胞们以此引起非常重视，希望大家有爱心的人传递下\n",
      "真实是谣言 预测是谣言 output=0.11\n",
      "---------------------------------------------\n",
      "@尾熊C 要提前预习育儿知识的话，建议看一些小巫写的书，嘻嘻\n",
      "真实是非谣言 预测是非谣言 output=0.54\n",
      "---------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "test_list = [\n",
    "    '兴仁县今天抢小孩没抢走，把孩子母亲捅了一刀，看见这车的注意了，真事，车牌号辽HFM055！！！！！赶紧散播！ 都别带孩子出去瞎转悠了 尤其别让老人自己带孩子出去 太危险了 注意了！！！！辽HFM055北京现代朗动，在各学校门口抢小孩！！！110已经 证实！！全市通缉！！',\n",
    "    '重庆真实新闻:2016年6月1日在重庆梁平县袁驿镇发生一起抢儿童事件，做案人三个中年男人，在三中学校到镇街上的一条小路上，把小孩直接弄晕(儿童是袁驿新幼儿园中班的一名学生)，正准备带走时被家长及时发现用棒子赶走了做案人，故此获救！请各位同胞们以此引起非常重视，希望大家有爱心的人传递下',\n",
    "    '@尾熊C 要提前预习育儿知识的话，建议看一些小巫写的书，嘻嘻',\n",
    "]\n",
    "test_label=[0,0,1]\n",
    "for i in range(len(test_list)):\n",
    "    predict_rumor_LSTM(test_list[i],test_label[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.2 64-bit ('tensorflow2': conda)",
   "language": "python",
   "name": "python36264bittensorflow2conda38b5dc13cee64c6696f4f4b70eb79ccb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
